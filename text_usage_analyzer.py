#!/usr/bin/env python3
"""
Ø£Ø¯Ø§Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ù…Ø´Ø±ÙˆØ¹ Flutter
ØªÙ‚ÙˆÙ… Ø¨ÙØ­Øµ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ¹Ø±Ù‘ÙØ© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø© ÙØ¹Ù„ÙŠØ§Ù‹
"""

import os
import re
import json
from typing import Set, Dict, List, Tuple

def extract_defined_texts(text_provider_path: str) -> Dict[str, Set[str]]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ¹Ø±Ù‘ÙØ© Ù…Ù† Ù…Ù„Ù TextProvider"""
    with open(text_provider_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù†ØµÙˆØµ
    texts_pattern = r"'([^']+)':\s*\{([^}]+)\}"
    matches = re.findall(texts_pattern, content, re.DOTALL)
    
    defined_texts = {}
    for lang, text_block in matches:
        keys = set()
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù…Ù† ÙƒØªÙ„Ø© Ø§Ù„Ù†Øµ
        key_pattern = r"'([^']+)':"
        text_keys = re.findall(key_pattern, text_block)
        keys.update(text_keys)
        defined_texts[lang] = keys
    
    return defined_texts

def extract_used_texts(lib_path: str) -> Set[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø© Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
    used_texts = set()
    
    for root, dirs, files in os.walk(lib_path):
        for file in files:
            if file.endswith('.dart'):
                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª getText
                    get_text_pattern = r"getText\(['\"]([^'\"]+)['\"]\)"
                    matches = re.findall(get_text_pattern, content)
                    used_texts.update(matches)
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª getTextWithParams
                    get_text_params_pattern = r"getTextWithParams\(['\"]([^'\"]+)['\"]\s*,"
                    matches = re.findall(get_text_params_pattern, content)
                    used_texts.update(matches)
                    
                except Exception as e:
                    print(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {e}")
    
    return used_texts

def analyze_text_usage(project_path: str) -> Dict:
    """ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
    text_provider_path = os.path.join(project_path, 'lib', 'presentation', 'providers', 'text_provider.dart')
    lib_path = os.path.join(project_path, 'lib')
    
    print("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ¹Ø±Ù‘ÙØ©...")
    defined_texts = extract_defined_texts(text_provider_path)
    
    print("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø©...")
    used_texts = extract_used_texts(lib_path)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    analysis = {}
    
    for lang, defined_keys in defined_texts.items():
        unused_texts = defined_keys - used_texts
        missing_texts = used_texts - defined_keys
        used_count = len(defined_keys & used_texts)
        
        analysis[lang] = {
            'total_defined': len(defined_keys),
            'total_used': used_count,
            'usage_percentage': (used_count / len(defined_keys)) * 100 if defined_keys else 0,
            'unused_texts': sorted(list(unused_texts)),
            'missing_texts': sorted(list(missing_texts)) if lang == 'ar' else []  # ÙÙ‚Ø· Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        }
    
    return analysis, used_texts

def generate_report(analysis: Dict, used_texts: Set[str], output_path: str):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµÙˆØµ\n\n")
        
        f.write("## Ù…Ù„Ø®Øµ Ø¹Ø§Ù…\n")
        f.write(f"- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø© ÙÙŠ Ø§Ù„ÙƒÙˆØ¯: {len(used_texts)}\n\n")
        
        for lang, data in analysis.items():
            lang_name = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" if lang == 'ar' else "Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
            f.write(f"## {lang_name} ({lang})\n")
            f.write(f"- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ¹Ø±Ù‘ÙØ©: {data['total_defined']}\n")
            f.write(f"- Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø©: {data['total_used']}\n")
            f.write(f"- Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {data['usage_percentage']:.1f}%\n\n")
            
            if data['unused_texts']:
                f.write(f"### Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…Ø© ({len(data['unused_texts'])}):\n")
                for text in data['unused_texts']:
                    f.write(f"- `{text}`\n")
                f.write("\n")
            
            if data['missing_texts']:
                f.write(f"### Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ({len(data['missing_texts'])}):\n")
                for text in data['missing_texts']:
                    f.write(f"- `{text}`\n")
                f.write("\n")

def main():
    project_path = r"C:\Users\MR_CODER\Desktop\media_player_app"
    output_path = os.path.join(project_path, "text_usage_report.md")
    
    print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
    
    try:
        analysis, used_texts = analyze_text_usage(project_path)
        
        print("\nğŸ“ˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
        for lang, data in analysis.items():
            lang_name = "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" if lang == 'ar' else "Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
            print(f"\n{lang_name}:")
            print(f"  - Ø§Ù„Ù…ÙØ¹Ø±Ù‘Ù: {data['total_defined']}")
            print(f"  - Ø§Ù„Ù…ÙØ³ØªØ®Ø¯Ù…: {data['total_used']}")
            print(f"  - Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…: {data['usage_percentage']:.1f}%")
            print(f"  - ØºÙŠØ± Ù…ÙØ³ØªØ®Ø¯Ù…: {len(data['unused_texts'])}")
            if data['missing_texts']:
                print(f"  - Ù…ÙÙ‚ÙˆØ¯: {len(data['missing_texts'])}")
        
        print(f"\nğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {output_path}")
        generate_report(analysis, used_texts, output_path)
        
        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")

if __name__ == "__main__":
    main()
